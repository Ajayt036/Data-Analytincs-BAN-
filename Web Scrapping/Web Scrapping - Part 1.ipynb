{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a836660a",
   "metadata": {},
   "source": [
    "# Web Scrapping - Part 1\n",
    "\n",
    "Source: https://realpython.com/python-web-scraping-practical-introduction/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d04494",
   "metadata": {},
   "source": [
    "Appendix:\n",
    "\n",
    "    \n",
    "1.  Urllib      : Library to open url\n",
    "2.  request     : A module in urllib library\n",
    "3.  urloprn     : A function in request module.\n",
    "4.  find        : An in-built function to find index first letter of any substring in a string\n",
    "5.  re.findall  : A f(x) of re (regular expression) library to find pattern of text based on metacharcter in a string\n",
    "6.  re.search   : A function of regular expession library to search a pattern of text in a sring\n",
    "7.  re.sub      :  A function of re to replace a text in a string\n",
    "8.  Beautiful soup : A library to parse a html file.\n",
    "9.  Soup        : An object used to craet in beautiful soup before starting parsing\n",
    "10. html.parser : Python's built in html parser. We can use this in our beuatiful soup\n",
    "11. MechanicalSoup: A library used to make headless browser to work with browsers\n",
    "12. time : Library to deal with timey ecxecution of commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3779ec50",
   "metadata": {},
   "source": [
    "## 1. What happens when we copy all data from a website and paste in a text file?\n",
    "\n",
    "All the data is pasted in simple text form. This is not the ideal way to scrap data because if all data is simple plain text, then how will we come to know which is header, which is sub header and which is  actuual content or headline in the webiste page. \n",
    "\n",
    "We should create first a html file and then scrap data from that file as per our need. Html of a website save this category in a certain manner under certain tag. such as title tag for headlines. in this case we extract the headline of webiste page using title tag from html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32d8b4",
   "metadata": {},
   "source": [
    "## 2. What is Web scrappinng?\n",
    "\n",
    "Web scraping is the process of collecting and parsing(analyzing text in a logical manner)  raw data from the Web\n",
    "\n",
    "Thre are following methods to scrap data from web:\n",
    "- Parse website data using string methods and regular expressions\n",
    "- Parse website data using an HTML parser\n",
    "- Interact with forms and other website components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33cd57f",
   "metadata": {},
   "source": [
    "## 3. Can a website put restriction on its data from being scrapped?\n",
    "\n",
    " Some websites explicitly forbid users from scraping their data with automated tools like the ones you’ll create in this tutorial. Websites do this for two possible reasons:\n",
    "- The site has a good reason to protect its data. For instance, Google Maps doesn’t let you request too many results too quickly.\n",
    "- Making many repeated requests to a website’s server may use up bandwidth, slowing down the website for other users and potentially overloading the server such that the website stops responding entirely.\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf56bd",
   "metadata": {},
   "source": [
    "## 4. Lets make our First Web Scrapper in 6 Steps.\n",
    "\n",
    "Urlib request > import urlopen > open url > real the html. bytes > decode the bytes in ot string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "967e2393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4\n",
      "<http.client.HTTPResponse object at 0x7f8ff8255e80> \n",
      "\n",
      "Step 5\n",
      "b'<html>\\n<head>\\n<title>Profile: Aphrodite</title>\\n</head>\\n<body bgcolor=\"yellow\">\\n<center>\\n<br><br>\\n<img src=\"/static/aphrodite.gif\" />\\n<h2>Name: Aphrodite</h2>\\n<br><br>\\nFavorite animal: Dove\\n<br><br>\\nFavorite color: Red\\n<br><br>\\nHometown: Mount Olympus\\n</center>\\n</body>\\n</html>\\n' \n",
      "\n",
      "Step 6 \n",
      " Final Output \n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<title>Profile: Aphrodite</title>\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br><br>\n",
      "<img src=\"/static/aphrodite.gif\" />\n",
      "<h2>Name: Aphrodite</h2>\n",
      "<br><br>\n",
      "Favorite animal: Dove\n",
      "<br><br>\n",
      "Favorite color: Red\n",
      "<br><br>\n",
      "Hometown: Mount Olympus\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "\n",
      " 1. These were steps to just scrap the html (string converted ) from url. Now we will read text from HTML\n",
      " 2. Note that utf-8 means changing the coding from bytes to string for human understanding\n"
     ]
    }
   ],
   "source": [
    "#1. Lets use urlib: an inbuilt library made to work with urls.\n",
    "\n",
    "#2. The urllib has module \"request\" that contains a function called urlopen() that can be used to open a URL.\n",
    "from urllib.request import urlopen\n",
    "\n",
    "#3 Select a url you want to open\n",
    "url = \"http://olympus.realpython.org/profiles/aphrodite\"\n",
    "\n",
    "#4 Use urlopen() command to open the url\n",
    "page = urlopen(url)\n",
    "print(\"Step 4\")\n",
    "print(page,\"\\n\")\n",
    "      \n",
    "      \n",
    "#5. Url will be saved an object in python. Use read() command to extract html from the webpage...\n",
    "#html is always saved in bytes.\n",
    "html_bytes = page.read()\n",
    "print(\"Step 5\")\n",
    "print(html_bytes,\"\\n\")\n",
    "\n",
    "#6 output gives us result in html bytes. With the hemp of decode(\"utf-8\"). we can gdecode the bytes to a string.\n",
    "html = html_bytes.decode(\"utf-8\")\n",
    "#Note that utf-8 means changing the coding from bytes to string for human understanding\n",
    "\n",
    "\n",
    "\n",
    "print(\"Step 6\",\"\\n\",\"Final Output\",\"\\n\")\n",
    "print(html)\n",
    "\n",
    "print(\"\\n\",\"1. These were steps to just scrap the html (string converted ) from url. Now we will read text from HTML\")\n",
    "print(\" 2. Note that utf-8 means changing the coding from bytes to string for human understanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c6298",
   "metadata": {},
   "source": [
    "## 5. Extract TITLE Text From HTML \n",
    "\n",
    "The < title > HTML element defines the document's title that is shown in a browser's title bar or a page's tab.\n",
    "    \n",
    "\n",
    "- HTML always save the title of the URL between title start and end i.e\"< title >\" and \"< /title >\" respectively\n",
    "- There are four  steps to find the title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c96d069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile: Aphrodite \n",
      "\n",
      "Note: In real word, Title will not be as clean as in this/. Because sometimes the exact substring <title> doesn’t exist and/it impact our slicing at Step 4.In these cases we have to sit and find solution for title with proper slicing\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Find starting index of titke using .find() function\n",
    "title_index = html.find(\"<title>\")\n",
    "title_index\n",
    "\n",
    "#Step 2: Adding len of <title> in step 1 output\n",
    "start_index = title_index + len(\"<title>\")\n",
    "start_index\n",
    "\n",
    "#Step 3: Last index of title end\n",
    "end_index = html.find(\"</title>\")\n",
    "end_index\n",
    "\n",
    "#Step 4: Slicing the Html to get title\n",
    "title = html[start_index:end_index]\n",
    "print(title,\"\\n\")\n",
    "\n",
    "print(\"Note: In real word, Title will not be as clean as in this/\\\n",
    ". Because sometimes the exact substring <title> doesn’t exist and/\\\n",
    "it impact our slicing at Step 4.In these cases we have to sit and find solution for title with proper slicing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd09c943",
   "metadata": {},
   "source": [
    "## 6. Extracting Text using Regular Expressions\n",
    "\n",
    "Regular expressions—or regexes for short—are patterns that can be used to search for text within a string. Python supports regular expressions through the standard library’s re module.\n",
    "\n",
    "Regular expressions use special characters called metacharacters to denote different patterns. For instance, the asterisk character (*) stands for zero or more of whatever comes just before the asterisk\n",
    "\n",
    "Before Starting extracting text from HTML First lets undedtand few function to extract text from strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa594ca",
   "metadata": {},
   "source": [
    "#### A.  Using findall Function from regular expression library\n",
    "\n",
    "Takes two argyment: 1. Text to be serached while logical expression, 2. String to be checked from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c72c7838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ac']\n",
      "Note: If match is not present, Empty list will be returned \n",
      "\n",
      "['ABC']\n",
      "['abcdefc']\n"
     ]
    }
   ],
   "source": [
    "#Lets find a text(with regulkar expression) from a string using findall function from regular expression (re) library\n",
    "import re\n",
    "\n",
    "\n",
    "#1. Use (*)\n",
    "#(*) stands for zero or more of whatever comes just before the asterisk.\n",
    "print(re.findall(\"ab*c\", \"ac\"))\n",
    "#\"ab*c\" can be seen as --\n",
    "#Pick part of string that starts with a\n",
    "#end with c\n",
    "#may or may not have b in between\n",
    "#You can see Some Extra Examples:\n",
    "re.findall(\"ab*c\", \"abcd\")\n",
    "re.findall(\"ab*c\", \"acc\")\n",
    "re.findall(\"ab*c\", \"abcac\")\n",
    "re.findall(\"ab*c\", \"abdc\")\n",
    "#In last exmaple. regular expession does have d do, nothing will return\n",
    "print(\"Note: If match is not present, Empty list will be returned\",\"\\n\")\n",
    "#Pattern matching is case sensitive\n",
    "print(re.findall(\"ab*c\", \"ABC\", re.IGNORECASE))\n",
    "\n",
    "\n",
    "#2. Use (.) \n",
    "# (.) stand for any single character in a regular expression.\n",
    "# Mean you could find all the strings that contain the letters \"a\" and \"c\" separated by a single character as follows:\n",
    "re.findall(\"a.c\", \"abc\")\n",
    "#You can see Some Extra Examples:\n",
    "re.findall(\"a.c\", \"abc\")\n",
    "re.findall(\"a.c\", \"abbc\")\n",
    "re.findall(\"a.c\", \"ac\")\n",
    "re.findall(\"a.c\", \"acc\")\n",
    "\n",
    "\n",
    "#3. Use(.*)\n",
    "# The pattern .* inside a regular expression stands for any character repeated any number of times\n",
    "print(re.findall(\"a.*c\", \"abcdefc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe05b1e",
   "metadata": {},
   "source": [
    "#### B. Using re.search( ) funtion\n",
    "\n",
    "This function is somewhat more complicated than re.findall() because it returns an object called a MatchObject that stores different groups of data. This is because there might be matches inside other matches, and re.search() returns every possible result.\n",
    "\n",
    "For now, just know that calling .group() on a MatchObject will return the first and most inclusive result, which in most cases is just what you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58ff260d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABC'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_results = re.search(\"ab*c\", \"ABC\", re.IGNORECASE)\n",
    "match_results\n",
    "match_results.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c8bc4",
   "metadata": {},
   "source": [
    "#### C. Using re.sub()\n",
    "\n",
    "is short for substitute, allows you to replace text in a string that matches a regular expression with new text. It behaves sort of like the .replace() string method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02fc2286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is ELEPHANTS.\n",
      "Note: Dont get surprised. Actually python replaced all values starting from first< to last>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Everything is ELEPHANTS if it's in ELEPHANTS.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Everything is <replaced> if it's in <tags>.\"\n",
    "string = re.sub(\"<.*>\", \"ELEPHANTS\", string)\n",
    "print(string)\n",
    "print(\"Note: Dont get surprised. Actually python replaced all values starting from first< to last>\")\n",
    "\n",
    "#If you want all < > values to be replaced separatly. Use operator \"?\"\n",
    "string = \"Everything is <replaced> if it's in <tags>.\"\n",
    "string = re.sub(\"<.*?>\", \"ELEPHANTS\", string)\n",
    "string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186679e4",
   "metadata": {},
   "source": [
    "## 7. Lets do an excercise to find title using Regular expression\n",
    "A. Provide the title of given url using re.research command\n",
    "\n",
    "url = \"http://olympus.realpython.org/profiles/dionysus\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14ce1bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<TITLE >Profile: Dionysus</title  / >\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br><br>\n",
      "<img src=\"/static/dionysus.jpg\" />\n",
      "<h2>Name: Dionysus</h2>\n",
      "<img src=\"/static/grapes.png\"><br><br>\n",
      "Hometown: Mount Olympus\n",
      "<br><br>\n",
      "Favorite animal: Leopard <br>\n",
      "<br>\n",
      "Favorite Color: Wine\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      " \n",
      "\n",
      "<TITLE >Profile: Dionysus</title  / > \n",
      "\n",
      "Title is:  Profile: Dionysus\n"
     ]
    }
   ],
   "source": [
    "#Lets First open a new url:\n",
    "from urllib.request import urlopen\n",
    "page = urlopen(\"http://olympus.realpython.org/profiles/dionysus\")\n",
    "html2 = page.read().decode(\"utf=8\")\n",
    "print(html2,\"\\n\")\n",
    "\n",
    "\n",
    "#Lets sreach Ttitle using re.search method.\n",
    "#We know re.sreach method tke three arguments: re.search(\"Pattern to find\", string , re.IGNORECASE)\n",
    "\n",
    "#Lets firts make a pattern to find the title:\n",
    "#The reason i am going to put .* in between title is in real world html is very messy between titles text.\n",
    "pattern = \"<title.*?>.*?</title.*?>\"\n",
    "\n",
    "#Now lets use this pattren in re.search function:\n",
    "import re\n",
    "title = re.search(pattern, html2, re.IGNORECASE )\n",
    "title = title.group()\n",
    "print(title,\"\\n\")\n",
    "\n",
    "#we got out title but if we see there is unwanted text in the title. Lets clean it by substuting unwanted text with \"\"\n",
    "clean_title = re.sub(\"<.*?>\", \"\", title)\n",
    "print(\"Title is: \", clean_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbbc6c",
   "metadata": {},
   "source": [
    "## 8. Extracting  Text from HTML\n",
    "\n",
    "Write a program that grabs the full HTML from the following URL. Then use .find() to display the text following “Name:” and “Favorite Color:” (not including any leading spaces or trailing HTML tags that might appear on the same line).\n",
    "\n",
    "URL = \"http://olympus.realpython.org/profiles/dionysus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11d237ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<TITLE >Profile: Dionysus</title  / >\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br><br>\n",
      "<img src=\"/static/dionysus.jpg\" />\n",
      "<h2>Name: Dionysus</h2>\n",
      "<img src=\"/static/grapes.png\"><br><br>\n",
      "Hometown: Mount Olympus\n",
      "<br><br>\n",
      "Favorite animal: Leopard <br>\n",
      "<br>\n",
      "Favorite Color: Wine\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      " \n",
      "\n",
      "Dionysus\n",
      "Wine\n"
     ]
    }
   ],
   "source": [
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "html = urlopen(url).read().decode(\"utf=8\")\n",
    "print(html,\"\\n\")\n",
    "\n",
    "#Lets use Loop for better way to asnwer this question\n",
    "for i in [\"Name: \", \"Favorite Color: \"]:\n",
    "    st_index = html.find(i) + len(i)\n",
    "    offset_index = html[st_index:].find(\"<\")\n",
    "    end_index = st_index + offset_index\n",
    "    print(html[st_index:end_index].strip(\" \\r\\n\\t\"))\n",
    "    #strip(\"\" \\r\\n\\t\"\") si to remove any space in rthe title. r,n,t are  are spaces in html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8496c0",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed66983",
   "metadata": {},
   "source": [
    "## 9. HTML Parser for Web Scraping \n",
    "\n",
    "Although regular expressions are great for pattern matching in general, sometimes it’s easier to use an HTML parser that’s explicitly designed for parsing out HTML pages. There are many Python tools written for this purpose, but the Beautiful Soup library is a good one to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f08a96c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 Output: \n",
      "\n",
      "\n",
      "Profile: Dionysus\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Name: Dionysus\n",
      "\n",
      "Hometown: Mount Olympus\n",
      "\n",
      "Favorite animal: Leopard \n",
      "\n",
      "Favorite Color: Wine\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Step 4 Output: \n",
      "<title>Profile: Dionysus</title>\n",
      "Profile: Dionysus \n",
      "\n",
      "Step 5 Output: \n",
      "[<img src=\"/static/dionysus.jpg\"/>, <img src=\"/static/grapes.png\"/>] \n",
      "\n",
      "Step 6 Output: \n",
      "img\n",
      "/static/dionysus.jpg\n",
      "[<img src=\"/static/dionysus.jpg\"/>] \n",
      "\n",
      "Step 7 Output: \n",
      "[]\n",
      "Note: Use href next to any single link to see its clean adress without html tags\n"
     ]
    }
   ],
   "source": [
    "#So far we have worked hard to get any text from html. we found the intial text, then index and then slice the html to\n",
    "#find any text and finally then clean it, removing all unwanted text using re.sub()\n",
    "#but there are many pasring tools which automatially gives us our text and that too after cleaning. Lets try them\n",
    "\n",
    "\n",
    "\n",
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "#Step 1 - Lets install Beautifulsoup from the Beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "#Step 2: Make a an object ( here we will mnake an object named soup)\n",
    "#Now lets make a soup object with BeautifulSoup \n",
    "#It will take two aruguments (html file to be parsed, And parser to be used on file)\n",
    "#our file name is html\n",
    "#We are using python's nuilt in  html.parser to parse our file \"html\"\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "#Step3 - Lets use this soup variable to get all text from our html using - .get_text()\n",
    "print(\"Step 3 Output: \")\n",
    "print(soup.get_text(),\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#Step 4 - Even a title can be extarcted usiing using title tag\n",
    "print(\"Step 4 Output: \")\n",
    "print(soup.title)\n",
    "#Cleaning the titke using .string\n",
    "print(soup.title.string,\"\\n\")\n",
    "\n",
    "\n",
    "#Step 5 - Find all images tag\n",
    "#sometimes the HTML tags themselves are the elements that point out the data you want to retrieve... \n",
    "#For instance, perhaps you want to retrieve the URLs for all the images on the page. These links...\n",
    "#are contained in the src attribute of <img> HTML tags\n",
    "print(\"Step 5 Output: \")\n",
    "print(soup.find_all(\"img\"),\"\\n\")\n",
    "#This returns a list of all <img> tags in the HTML document. We can see in...\n",
    "#Output one there are two images in this html. one is with source(src) jpg and other png\n",
    "\n",
    "\n",
    "\n",
    "#Step 6- Exploring the img tag:\n",
    "#from the list ourput of Step 5, we can see how many iamges are there in html\n",
    "#We can unpact all these img differently, and then can explore a partcular image such as check its source\n",
    "image1, image2 = soup.find_all(\"img\")\n",
    "#Checking name tag\n",
    "print(\"Step 6 Output: \")\n",
    "print(image1.name)\n",
    "print(image1[\"src\"])\n",
    "#We can even search any tag with spefici source\n",
    "print(soup.find_all(\"img\", src=\"/static/dionysus.jpg\"),\"\\n\")\n",
    "\n",
    "\n",
    "#Step 7: To get all links stored links html file.\n",
    "print(\"Step 7 Output: \")\n",
    "print(soup.find_all(\"a\"))\n",
    "#Return emplty list if there is on link\n",
    "print(\"Note: Use href next to any single link to see its clean adress without html tags\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300b515",
   "metadata": {},
   "source": [
    "## 10. An Excercie for Beautifulsoup\n",
    "Write a program that grabs the full HTML from the page at the URL http://olympus.realpython.org/profiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "34ec3b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>All Profiles</title>\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br><br>\n",
      "<h1>All Profiles:</h1>\n",
      "<br><br>\n",
      "<h2>\n",
      "<a href=\"/profiles/aphrodite\">Aphrodite</a>\n",
      "<br><br>\n",
      "<a href=\"/profiles/poseidon\">Poseidon</a>\n",
      "<br><br>\n",
      "<a href=\"/profiles/dionysus\">Dionysus</a>\n",
      "</h2>\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "[<a href=\"/profiles/aphrodite\">Aphrodite</a>, <a href=\"/profiles/poseidon\">Poseidon</a>, <a href=\"/profiles/dionysus\">Dionysus</a>] \n",
      "\n",
      "/profiles/aphrodite\n",
      "/profiles/poseidon\n",
      "/profiles/dionysus\n",
      "\n",
      "http://olympus.realpython.org/profiles/profiles/aphrodite\n",
      "http://olympus.realpython.org/profiles/profiles/poseidon\n",
      "http://olympus.realpython.org/profiles/profiles/dionysus\n"
     ]
    }
   ],
   "source": [
    "#Lets frirst open url and convert it into a html\n",
    "from urllib.request import urlopen\n",
    "url = \"http://olympus.realpython.org/profiles\"\n",
    "html = urlopen(url).read().decode(\"utf = 8\")\n",
    "print(html)\n",
    "\n",
    "\n",
    "#Lets make a soup object to get our text from html\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#Finding all links stored in given url\n",
    "print(soup.find_all(\"a\"),\"\\n\")\n",
    "#But the output og links are in html tag. if we want to remove the tags from link if we can use [\"href\"]...\n",
    "#to remove tags\n",
    "\n",
    "for i in (soup.find_all(\"a\")):\n",
    "    print(i[\"href\"])\n",
    "#Now the poutput is clean.\n",
    "print(\"\")\n",
    "\n",
    "#To provide full link ,we can add our url infront of these other links to comlete it\n",
    "for link in soup.find_all(\"a\"):\n",
    "    link_url = url + link[\"href\"]\n",
    "    print(link_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331f7a2",
   "metadata": {},
   "source": [
    "## 11. Interact With HTML Forms: A little Hacking\n",
    "\n",
    "Sometimes, though, you need to interact with a web page to obtain the content you need. For example, you might need to submit a form or click a button to display hidden content.Many third-party packages are available from PyPI for working with web pages interactively. Among these, MechanicalSoup is a popular and relatively straightforward package to use.\n",
    "\n",
    "In essence, MechanicalSoup installs what’s known as a headless browser, which is a web browser with no graphical user interface. This browser is controlled programmatically via a Python program.\n",
    "\n",
    "**Lets use it to Automatically Login in a website**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "34b70a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 Output: \n",
      "<Response [200]>\n",
      "Note: 200 means request has been accepted by the link \n",
      "\n",
      "Step 3 Output: \n",
      "<class 'bs4.BeautifulSoup'>\n",
      "Step 3 Output: \n",
      "<html>\n",
      "<head>\n",
      "<title>Log In</title>\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br/><br/>\n",
      "<h2>Please log in to access Mount Olympus:</h2>\n",
      "<br/><br/>\n",
      "<form action=\"/login\" method=\"post\" name=\"login\">\n",
      "Username: <input name=\"user\" type=\"text\"/><br/>\n",
      "Password: <input name=\"pwd\" type=\"password\"/><br/><br/>\n",
      "<input type=\"submit\" value=\"Submit\"/>\n",
      "</form>\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "Step 5 Output: \n",
      "<form action=\"/login\" method=\"post\" name=\"login\">\n",
      "Username: <input name=\"user\" type=\"text\"/><br/>\n",
      "Password: <input name=\"pwd\" type=\"password\"/><br/><br/>\n",
      "<input type=\"submit\" value=\"Submit\"/>\n",
      "</form> \n",
      "\n",
      "Step 8 Output: \n",
      "http://olympus.realpython.org/profiles\n",
      "Open this link in chrome to see your reslt page after login \n",
      "\n",
      "Step 9 Output: \n",
      "http://olympus.realpython.org/login/profiles/aphrodite\n",
      "http://olympus.realpython.org/login/profiles/poseidon\n",
      "http://olympus.realpython.org/login/profiles/dionysus\n"
     ]
    }
   ],
   "source": [
    "#1. Create Headless Browser object using Mechanical Soup.\n",
    "import mechanicalsoup\n",
    "browser = mechanicalsoup.Browser()\n",
    "\n",
    "\n",
    "\n",
    "#2. Take a login URL and put the URl in pur crated browser\n",
    "url = \"http://olympus.realpython.org/login\"\n",
    "page = browser.get(url)\n",
    "print(\"Step 2 Output: \")\n",
    "print(page)\n",
    "print(\"Note: 200 means request has been accepted by the link\",\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#3. Look at Automatic created soup and page from bcreated browser above\n",
    "#MechanicalSoup uses Beautiful Soup to parse the HTML from the request. page has a .soup attribute...\n",
    "#that represents a BeautifulSoup object:\n",
    "print(\"Step 3 Output: \")\n",
    "print(type(page.soup))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#4. Loolking at HTML of login page using .soup\n",
    "#Note that we know abovbe link is a log in link. Hence we will see form tag in the HTML\n",
    "html = page.soup\n",
    "print(\"Step 3 Output: \")\n",
    "print(html)\n",
    "\n",
    "\n",
    "#5. See Form: .select(\"form\") returns a list of all <form> elements on the page such as username and password.\n",
    "form = html.select(\"form\")[0]\n",
    "print(\"Step 5 Output: \")\n",
    "print(form,\"\\n\")\n",
    "\n",
    "\n",
    "#6 Filling the credentials in Form Elemnts.\n",
    "form.select(\"input\")[0][\"value\"] = \"zeus\"\n",
    "form.select(\"input\")[1][\"value\"] = \"ThunderDude\"\n",
    "\n",
    "\n",
    "#7. Submitting the form\n",
    "Result_page = browser.submit(form, page.url)\n",
    "\n",
    "\n",
    "\n",
    "#8. Opening the Reuslt that we get from sumitting the form by running .url\n",
    "print(\"Step 8 Output: \")\n",
    "print(Result_page.url)\n",
    "print(\"Open this link in chrome to see your reslt page after login\",\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#9. Getting all links from Resu;yt_Page\n",
    "print(\"Step 9 Output: \")\n",
    "links = Result_page.soup.select(\"a\")\n",
    "for i in links:\n",
    "    print(url + i[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97117f52",
   "metadata": {},
   "source": [
    "## 12. Interact With Websites in Real Time\n",
    "\n",
    "Sometimes you want to be able to fetch real-time data from a website that offers continually updated information. \n",
    "\n",
    "In the dark days before you learned Python programming, you had to sit in front of a browser, clicking the Refresh button to reload the page each time you wanted to check if updated content was available. But now you can automate this process using the .get() method of the MechanicalSoup Browser object.\n",
    "\n",
    "url = http://olympus.realpython.org/dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e26db30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of your dice roll is: 3\n",
      "I'm about to wait for 2 seconds...\n",
      "Done waiting!\n",
      "The result of your dice roll is: 1\n",
      "The result of your dice roll is: 5\n",
      "The result of your dice roll is: 1\n"
     ]
    }
   ],
   "source": [
    "#1 - Lets call above url and see what result it gives in first instance.\n",
    "import mechanicalsoup\n",
    "browser = mechanicalsoup.Browser()\n",
    "page = browser.get(\"http://olympus.realpython.org/dice\")\n",
    "tag = page.soup.select(\"#result\")[0]\n",
    "result = tag.text\n",
    "print(f\"The result of your dice roll is: {result}\")\n",
    "\n",
    "\n",
    "\n",
    "#2 - Now lets import time library and see how it work with simple example\n",
    "import time\n",
    "print(\"I'm about to wait for 2 seconds...\")\n",
    "time.sleep(2)\n",
    "print(\"Done waiting!\")\n",
    "\n",
    "\n",
    "# Now lets combine step 1 &2 - see automatic refresh after every 3 seonds for 4 intsnaces.\n",
    "for i in range(3):\n",
    "    page = browser.get(\"http://olympus.realpython.org/dice\")\n",
    "    tag = page.soup.select(\"#result\")[0]\n",
    "    result = tag.text\n",
    "    print(f\"The result of your dice roll is: {result}\")\n",
    "    time.sleep(3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
